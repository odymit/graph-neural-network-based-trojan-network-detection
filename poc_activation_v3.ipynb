{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# POC 激活验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "from utils_gnn import cnn2graph_activation\n",
    "# from model_lib import mnist_cnn_model as father_model\n",
    "from utils_basic import load_spec_model\n",
    "from utils_gnn import padding, unpadding\n",
    "from dgl.data import DGLDataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dgl import save_graphs, load_graphs\n",
    "import torch\n",
    "import json\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "# from model_lib.mnist_cnn_model import Model\n",
    "from random import randint\n",
    "from utils_basic import load_spec_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling, SortPooling\n",
    "from utils_gnn import MLP\n",
    "\n",
    "\n",
    "# x = '/home/dorian/repos/Meta-Nerual-Trojan-Detection/shadow_model_ckpt/mnist/models5/shadow_jumbo_9.model'\n",
    "x = './shadow_model_ckpt/mnist/models5/shadow_jumbo_9.model'\n",
    "# load model \n",
    "# Model = load_spec_model(father_model, '5')\n",
    "from model_lib.mnist_cnn_model import Model6 as Model\n",
    "model = Model(gpu=True)\n",
    "params = torch.load(x)\n",
    "model.load_state_dict(params)\n",
    "del params\n",
    "\n",
    "# load model detail \n",
    "model_detail = {}\n",
    "model_detail_path = \"./intermediate_data/model_detail.json\"\n",
    "import json\n",
    "with open(model_detail_path, 'r') as f:\n",
    "    model_detail = json.load(f)\n",
    "# print(model_detail)\n",
    "g = cnn2graph_activation(model, model_detail['mnist']['5'])\n",
    "dgl.save_graphs('./intermediate_data/grapj_test.bin', g)\n",
    "del model_detail\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# from utils_gnn import SGNACT\n",
    "GPU = True\n",
    "if GPU:\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "BATCH_SIZE = 1\n",
    "# MNIST image dataset \n",
    "trainset = torchvision.datasets.MNIST(root='./raw_data/', train=True, download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# get a image\n",
    "image = None\n",
    "label = None\n",
    "for i, (x_in, y_in) in enumerate(dataloader):\n",
    "    image = x_in\n",
    "    model(image)\n",
    "    label = y_in\n",
    "    break\n",
    "del trainset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the init process\n",
    "cnt = 0 \n",
    "def init_conv(data, data_size, weight, bias, kernel_size, stride, padding):\n",
    "    global cnt\n",
    "    row, col = weight.size()\n",
    "    # print(weight.size())\n",
    "    # get actual conv kernel weight and bias\n",
    "    w = unpadding(weight, 1, kernel_size[0]*kernel_size[1])\n",
    "    w = w.reshape(1, 1, kernel_size[0], kernel_size[1])\n",
    "    ws = w\n",
    "    np.savetxt(\"./intermediate_data/init/weight-%d.csv\" % cnt, ws[0][0].cpu().numpy(), delimiter=',')\n",
    "    cnt += 1\n",
    "    b = unpadding(bias, 1, 1)[0]\n",
    "    np.savetxt(\"./intermediate_data/init/bias-%d.csv\" % cnt, b.cpu().numpy(), delimiter=',')\n",
    "    # get conv operator \n",
    "    # print(\"kernel_size, stride, padding:\")\n",
    "    # print(kernel_size, stride, padding)\n",
    "    operator = torch.nn.Conv2d(1, 1, kernel_size=kernel_size, \n",
    "                    stride=stride, padding=padding)\n",
    "    # set conv operator weight and bias\n",
    "    operator.weight.data = w\n",
    "    operator.bias.data = b\n",
    "    # conduct conv operation\n",
    "    # print(\"conv input size:\", data.size())\n",
    "    x = operator(data.to(\"cuda\"))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def initiate_node_feature(graph, image):\n",
    "    ft = None \n",
    "    mask = graph.ndata['tag'] == 0\n",
    "    out_channels = int(sum(mask))\n",
    "    ft = torch.zeros((len(graph.nodes()), 1, 1, 28, 28))\n",
    "    convd_size = torch.zeros((len(graph.nodes()), 2))\n",
    "    for i in range(out_channels):\n",
    "        kernel_size, stride, padding = graph.ndata['kernel_params'][i]\n",
    "        # do conv\n",
    "        res_ft = init_conv(image, None, graph.ndata['kernel_weight'][i], graph.ndata['bias'][i],\n",
    "                kernel_size, stride, padding)\n",
    "        # do relu\n",
    "        relu_opt = torch.nn.functional.relu\n",
    "        res_ft = relu_opt(res_ft)\n",
    "\n",
    "        # do max pooling\n",
    "        pooling = graph.ndata['pooling_params'][i]\n",
    "        if pooling.all() != 0:\n",
    "            # do max_pooling\n",
    "            kernel_size, stride, pad, dilation, ceil_mode = pooling\n",
    "            max_pooling_operator = torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, \n",
    "                padding=pad, dilation=dilation, ceil_mode=ceil_mode)\n",
    "            \n",
    "            res_ft = max_pooling_operator(res_ft)\n",
    "        _, _, r, c = res_ft.size()\n",
    "        ft[i] = res_ft\n",
    "        convd_size[i] = torch.tensor([int(r), int(c)])\n",
    "    graph.ndata['ft'] = ft.to(\"cuda\")\n",
    "    graph.ndata['ft_size'] = convd_size.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3],\n",
      "       device='cuda:0')\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
      "        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# init ft feature and get subg\n",
    "with torch.no_grad():\n",
    "    initiate_node_feature(g, image)\n",
    "print(g.ndata['tag'])\n",
    "# get node id\n",
    "def nodes_with_feature_smaller_two(nodes):\n",
    "    return nodes.data['tag'] <= 1\n",
    "nodes_idx = g.filter_nodes(nodes_with_feature_smaller_two)\n",
    "# print(nodes_idx)\n",
    "subg = dgl.node_subgraph(g, nodes_idx, relabel_nodes=True)\n",
    "print(subg.nodes())\n",
    "ft32 = None\n",
    "# got the right subg of conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils_gnn import equals, prepare_data\n",
    "import pdb\n",
    "\n",
    "def prepare_conv_data(nkernel_weight, \n",
    "                      nkernel_size, \n",
    "                      nkernel_bias, \n",
    "                      nkernel_params, \n",
    "                      channels):\n",
    "    # prepare conv kernel weight\n",
    "    _, width, height = nkernel_size\n",
    "    kernel_weight = nkernel_weight\n",
    "    kernel_weight = unpadding(kernel_weight, channels, width * height)\n",
    "    kernel_weight = kernel_weight.reshape(1, channels, width, height)\n",
    "    # prepare conv bias\n",
    "    bias = nkernel_bias\n",
    "    kernel_bias = unpadding(bias, 1, 1)[0]\n",
    "\n",
    "    # prepare conv operator\n",
    "    kernel_size, stride, pad = nkernel_params\n",
    "\n",
    "    return kernel_weight, kernel_bias, kernel_size, stride, pad\n",
    "    \n",
    "def do_operation(data, conv_opt, pooling_params):\n",
    "    # do conv\n",
    "    torch.save(data, \"./intermediate_data/reshaped_data.pt\")\n",
    "    conv_ft = conv_opt(data.to(\"cuda\")).to(\"cuda\")\n",
    "    _, _, width, height = conv_ft.size()\n",
    "    np.savetxt(\"./intermediate_data/process/conv_processed_ft.csv\", conv_ft.reshape(width, height).cpu().numpy(), delimiter=',')\n",
    "    # do relu\n",
    "    relu_ft = torch.nn.functional.relu(conv_ft)\n",
    "    np.savetxt(\"./intermediate_data/process/relu_processed_ft.csv\", relu_ft.reshape(width, height).cpu().numpy(), delimiter=',')\n",
    "    ret_ft = relu_ft\n",
    "    # do pooling\n",
    "    if pooling_params.any() != 0:\n",
    "        kernel_size, stride, pad, dilation, ceil_mode = pooling_params\n",
    "        kernel_size, stride, pad, dilation, ceil_mode = int(kernel_size), int(stride), int(pad), int(dilation), bool(ceil_mode)\n",
    "        max_pooling_operator = torch.nn.MaxPool2d(kernel_size=kernel_size, stride=stride, \n",
    "            padding=pad, dilation=dilation, ceil_mode=ceil_mode)\n",
    "        \n",
    "        ret_ft = max_pooling_operator(relu_ft)\n",
    "        _, _, width, height = ret_ft.size()\n",
    "        # np.savetxt(\"./intermediate_data/ret_ft.csv\", ret_ft.reshape(width, height).cpu().numpy(), delimiter=',')\n",
    "\n",
    "    return ret_ft\n",
    "\n",
    "\n",
    "\n",
    "def my_reduce(nodes):\n",
    "    # received data and their size\n",
    "    mfeat = nodes.mailbox['m']\n",
    "    msize = nodes.mailbox['n']\n",
    "    \n",
    "    mf_size = mfeat.size()\n",
    "    n_nodes = mf_size[0]\n",
    "    n_channels = mf_size[1]\n",
    "\n",
    "    # prepare ret_ft & ret_size, input size (1, 1, 28, 28)\n",
    "    ret_ft = torch.zeros((n_nodes, 1, 1, 28, 28)).to(\"cuda\")\n",
    "    ret_size = torch.zeros((n_nodes, 2)).to(\"cuda\")\n",
    "\n",
    "    # for each node, do conv for received data\n",
    "    for out_idx in range(n_nodes):\n",
    "\n",
    "        # prepare received data and conv weight\n",
    "        received_data = mfeat[out_idx]\n",
    "        received_size = msize[out_idx]\n",
    "        np.savetxt(\"./intermediate_data/process/rec-size-%d.csv\" % out_idx, received_size.cpu().numpy(), delimiter=',')\n",
    "        torch.save(received_data, \"./intermediate_data/process/receivec_data.pt\")\n",
    "\n",
    "        # skip if all zeros\n",
    "        if equals(received_data, zeros=True):\n",
    "            continue\n",
    "        # (n, 1, 1, 28, 28) -> (1, n, width, height)\n",
    "        data = prepare_data(received_data, received_size)\n",
    "\n",
    "        # prepare conv operator data\n",
    "        kernel_weight, kernel_bias, kernel_size, stride, pad = prepare_conv_data(nodes.data['kernel_weight'][out_idx], \n",
    "                                                                                nodes.data['kernel_size'][out_idx],\n",
    "                                                                                nodes.data['bias'][out_idx], \n",
    "                                                                                nodes.data['kernel_params'][out_idx], \n",
    "                                                                                n_channels)\n",
    "        \n",
    "        # prepare conv operator\n",
    "        conv_opt = torch.nn.Conv2d(n_channels, 1, kernel_size=kernel_size, stride=stride, padding=pad)\n",
    "        conv_opt.weight.data = kernel_weight\n",
    "        conv_opt.bias.data = kernel_bias\n",
    "        torch.save(kernel_weight, \"./intermediate_data/process/kernel_weight.pt\")\n",
    "        np.savetxt(\"./intermediate_data/process/kernel-bias-%d.csv\" % (out_idx),  kernel_bias.cpu().numpy(), delimiter=',')\n",
    "        # print(\"kernel weight size:\", kernel_weight.size(), kernel_size)\n",
    "        # print(\"kernel_bias size:\", kernel_size.size())\n",
    "\n",
    "        # do operation\n",
    "        ft = do_operation(data, conv_opt, nodes.data['pooling_params'][out_idx])\n",
    "\n",
    "        # save ft size\n",
    "        _, _, width, height = ft.size()\n",
    "        np.savetxt(\"./intermediate_data/process/ret_ft.csv\", ft.reshape(width, height).cpu().numpy(), delimiter=',')\n",
    "        pad_ft = padding(ft.reshape(width, height), 28, 28)\n",
    "        np.savetxt(\"./intermediate_data/process/pad_ft.csv\", pad_ft.cpu().numpy(), delimiter=',')\n",
    "        reshaped_ft = pad_ft.reshape(1, 1, 28, 28)\n",
    "        # update return data\n",
    "        ret_ft[out_idx] = reshaped_ft\n",
    "        ret_size[out_idx] = torch.tensor([width, height]).to(\"cuda\")\n",
    "        # a_break = input(\"go next?\")\n",
    "        \n",
    "    # return size is [n, 1, 1, 28, 28], reduced from [n, m, 1, 1, 28, 28]\n",
    "    return {'h': ret_ft, 'g': ret_size}\n",
    "\n",
    "def my_message(edges):\n",
    "    return {'m': edges.src['ft'], 'n': edges.src['ft_size']}\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    subg.update_all(my_message, my_reduce)\n",
    "    subg.ndata['ft'] += subg.ndata['h']\n",
    "    subg.ndata['ft_size'] += subg.ndata['g']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88, 1, 1, 28, 28])\n",
      "torch.Size([88, 2])\n"
     ]
    }
   ],
   "source": [
    "print(subg.ndata['ft'].size())\n",
    "print(subg.ndata['ft_size'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft: \n",
      "zeros -  [40, 44, 45, 47, 48, 54, 58, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "nonzeros -  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 49, 50, 51, 52, 53, 55, 56, 57, 59, 60, 61, 62]\n",
      "h: \n",
      "zeros -  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 40, 44, 45, 47, 48, 54, 58, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "nonzeros -  [32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 46, 49, 50, 51, 52, 53, 55, 56, 57, 59, 60, 61, 62]\n",
      "ft_size: \n",
      "zeros -  [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "nonzeros -  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "g: \n",
      "zeros -  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "nonzeros -  [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n"
     ]
    }
   ],
   "source": [
    "def print_zeros_in_feat(key):\n",
    "    print(key + \": \")\n",
    "    zeros = []\n",
    "    nonzero = []\n",
    "    for id in subg.nodes():\n",
    "        id = int(id)\n",
    "        feat = subg.nodes[id].data[key]\n",
    "        if torch.eq(feat, torch.zeros(feat.size()).to(\"cuda\")).all() == True:\n",
    "            zeros.append(id)\n",
    "        else:\n",
    "            nonzero.append(id)\n",
    "    print(\"zeros - \", zeros)\n",
    "    print(\"nonzeros - \", nonzero)\n",
    "\n",
    "print_zeros_in_feat('ft')\n",
    "print_zeros_in_feat('h')\n",
    "print_zeros_in_feat('ft_size')\n",
    "print_zeros_in_feat('g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "ft_list = []\n",
    "for f,s in zip(subg.ndata['ft'], subg.ndata['ft_size']):\n",
    "    w, h = s\n",
    "    w, h = int(w), int(h)\n",
    "    data = unpadding(f.reshape(28, 28), w, h)\n",
    "    ft_list.append(data)\n",
    "    np.savetxt(\"./intermediate_data/graph_results/node-%d.csv\" % cnt, data.cpu(), delimiter=',')\n",
    "    cnt += 1\n",
    "\n",
    "cnt = 0\n",
    "cnn_params = None \n",
    "with open(\"./intermediate_data/params.json\", \"r\") as f:\n",
    "    cnn_params = json.load(f)\n",
    "from torch.nn.functional import relu\n",
    "from utils_gnn import unpadding\n",
    "conv1 = torch.tensor(cnn_params['conv1'])\n",
    "conv2 = torch.tensor(cnn_params['conv2'])\n",
    "conv3 = torch.tensor(cnn_params['conv3'])\n",
    "conv4 = torch.tensor(cnn_params['conv4'])\n",
    "l = [conv1, conv2, conv3, conv4]\n",
    "neural_list = []\n",
    "for layer in l:\n",
    "    for neural in layer[0]:\n",
    "        neural_list.append(neural)\n",
    "        np.savetxt(\"./intermediate_data/model_results/node-%d.csv\" % cnt, neural.cpu(), delimiter=',')\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "tensor([[[[3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.9098e-02, 4.0387e-02, 3.1758e-02, 4.2234e-02, 7.2317e-02,\n",
      "           2.8933e-03, 7.3194e-02, 5.3073e-02, 2.4708e-01, 8.9706e-02,\n",
      "           7.7158e-02, 1.1589e-01, 2.3346e-01, 2.6848e-01, 1.4724e-01,\n",
      "           7.3427e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 4.5209e-02, 2.5505e-02, 5.3447e-02, 3.9736e-02,\n",
      "           4.6416e-02, 1.0554e-01, 1.0643e-01, 1.9159e-01, 1.4959e-01,\n",
      "           2.5495e-01, 3.2515e-01, 3.8833e-01, 3.1328e-01, 1.1457e-01,\n",
      "           4.4804e-01, 4.2849e-01, 4.8771e-01, 2.8495e-01, 1.5594e-01,\n",
      "           1.2148e-01, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           4.9509e-02, 3.9924e-02, 0.0000e+00, 8.1022e-02, 1.5740e-01,\n",
      "           3.0953e-01, 2.8452e-01, 3.4169e-01, 4.3153e-01, 3.4424e-01,\n",
      "           3.1003e-01, 2.5527e-01, 4.3615e-01, 1.6389e-01, 5.2248e-02,\n",
      "           0.0000e+00, 1.2723e-01, 1.0586e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           1.3154e-02, 0.0000e+00, 0.0000e+00, 2.1056e-01, 1.6607e-01,\n",
      "           2.5030e-01, 2.4320e-01, 1.6086e-01, 1.3771e-01, 2.2571e-01,\n",
      "           1.6717e-01, 3.0787e-01, 2.4505e-01, 0.0000e+00, 5.2664e-02,\n",
      "           8.3668e-02, 1.0504e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           8.5862e-03, 0.0000e+00, 0.0000e+00, 7.5397e-02, 2.1124e-01,\n",
      "           1.0136e-01, 2.4859e-01, 3.1557e-01, 4.0609e-01, 5.2857e-02,\n",
      "           3.5600e-02, 9.2219e-02, 1.5687e-01, 0.0000e+00, 0.0000e+00,\n",
      "           1.7774e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           7.6417e-02, 1.3170e-01, 2.0165e-01, 2.8643e-01, 1.0505e-01,\n",
      "           0.0000e+00, 0.0000e+00, 1.1780e-01, 6.3831e-02, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           1.6069e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           2.1836e-02, 8.3105e-02, 2.6778e-01, 3.1385e-01, 5.0722e-02,\n",
      "           0.0000e+00, 0.0000e+00, 7.1020e-02, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 8.8565e-03, 0.0000e+00, 8.6456e-03, 2.4799e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           2.6255e-02, 0.0000e+00, 1.6475e-01, 2.1520e-01, 8.8346e-02,\n",
      "           0.0000e+00, 5.2387e-02, 7.6655e-02, 0.0000e+00, 0.0000e+00,\n",
      "           3.4586e-02, 1.8601e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 0.0000e+00, 0.0000e+00, 5.1356e-02, 0.0000e+00,\n",
      "           0.0000e+00, 4.2358e-02, 1.6310e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 3.2039e-01, 1.1591e-01, 1.3150e-02, 0.0000e+00,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 2.8958e-02, 4.3899e-02, 5.9011e-02,\n",
      "           1.7085e-01, 1.8777e-01, 5.2952e-02, 0.0000e+00, 0.0000e+00,\n",
      "           1.6740e-02, 3.1103e-01, 2.5908e-01, 2.1715e-01, 7.9756e-02,\n",
      "           4.5310e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 0.0000e+00,\n",
      "           9.7975e-02, 2.5023e-01, 9.4120e-02, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 1.7216e-01, 2.0751e-01, 2.4691e-01, 2.1731e-01,\n",
      "           1.0130e-01, 4.5861e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.0985e-02,\n",
      "           0.0000e+00, 1.6864e-01, 3.4687e-01, 6.4558e-02, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 1.5144e-02, 4.3802e-02, 2.6666e-01,\n",
      "           2.0629e-01, 1.0387e-01, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           1.4767e-02, 0.0000e+00, 2.6024e-01, 3.6341e-01, 1.5357e-01,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6988e-01,\n",
      "           1.0241e-01, 2.0094e-01, 5.6061e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 0.0000e+00, 0.0000e+00, 1.7455e-01, 2.9248e-01,\n",
      "           1.5966e-01, 3.1980e-02, 0.0000e+00, 0.0000e+00, 1.0936e-01,\n",
      "           0.0000e+00, 9.7067e-03, 7.1935e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           4.7246e-02, 4.4537e-02, 0.0000e+00, 0.0000e+00, 1.5987e-01,\n",
      "           4.0884e-01, 5.1655e-01, 4.5846e-01, 2.5179e-01, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 4.3851e-02, 4.7372e-02,\n",
      "           2.7503e-04, 0.0000e+00, 9.0419e-02, 2.1454e-01, 1.7038e-01,\n",
      "           2.8917e-01, 5.2804e-01, 6.4451e-01, 4.4980e-01, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 5.1879e-03, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 4.3625e-02, 3.7210e-02, 4.0510e-02, 0.0000e+00,\n",
      "           3.8931e-02, 1.8128e-01, 2.4735e-01, 3.0106e-01, 4.4017e-01,\n",
      "           3.6971e-01, 4.0663e-01, 1.4283e-01, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 1.5084e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 4.2493e-02,\n",
      "           6.4486e-02, 0.0000e+00, 3.3832e-02, 4.8920e-02, 1.8729e-01,\n",
      "           2.9119e-01, 2.9776e-01, 4.6673e-01, 5.4292e-01, 4.0203e-01,\n",
      "           8.9289e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 3.7720e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 5.0867e-02, 3.8736e-02, 0.0000e+00,\n",
      "           0.0000e+00, 1.3490e-01, 3.0667e-01, 2.4234e-01, 3.2759e-01,\n",
      "           4.0485e-01, 5.5799e-01, 4.5710e-01, 1.5930e-01, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.6269e-02, 0.0000e+00, 3.7648e-02,\n",
      "           1.7395e-01, 1.7195e-01, 2.6210e-01, 4.5474e-01, 3.6120e-01,\n",
      "           4.2288e-01, 2.4029e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           1.1180e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 0.0000e+00, 0.0000e+00, 1.7380e-02,\n",
      "           1.3809e-01, 2.6686e-01, 3.2396e-01, 5.3370e-02, 2.2327e-04,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6082e-03, 3.7720e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.6039e-02, 6.5298e-02, 1.1945e-01,\n",
      "           1.0054e-01, 3.9980e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 8.1236e-03, 3.5276e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 1.2614e-01, 2.8428e-01, 4.2178e-01,\n",
      "           3.6498e-01, 1.5731e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           3.4577e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2831e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02],\n",
      "          [3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02, 3.8419e-02,\n",
      "           3.8419e-02, 3.8419e-02, 3.8419e-02]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(g.ndata['ft'][0].size())\n",
    "print(g.ndata['ft'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "tensor([[0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1077, 0.1135, 0.1277, 0.1332, 0.1728, 0.2831, 0.3180, 0.4338,\n",
      "         0.4112, 0.5794, 0.5554, 0.5856, 0.6266, 0.5389, 0.3915, 0.1871, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1133, 0.1420, 0.1644,\n",
      "         0.2614, 0.3535, 0.4540, 0.6250, 0.7038, 0.7907, 0.7981, 0.8490, 0.8930,\n",
      "         0.9739, 0.8651, 0.7577, 0.8435, 0.9228, 0.8003, 0.4697, 0.1965, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1172, 0.1965, 0.3658, 0.4665,\n",
      "         0.6777, 0.8325, 0.8884, 0.9073, 0.9810, 0.9928, 0.9427, 0.8033, 0.7155,\n",
      "         0.6964, 0.4217, 0.1239, 0.1426, 0.3109, 0.3777, 0.2772, 0.1423, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1027, 0.0943, 0.2060, 0.3768,\n",
      "         0.6729, 0.8232, 0.7589, 0.6804, 0.5759, 0.5764, 0.5532, 0.5495, 0.5050,\n",
      "         0.5478, 0.0663, 0.0000, 0.0000, 0.2244, 0.3415, 0.2631, 0.1664, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.0824, 0.0000, 0.0000, 0.0000,\n",
      "         0.0278, 0.4188, 0.4721, 0.5145, 0.5098, 0.3713, 0.0961, 0.0734, 0.3468,\n",
      "         0.4971, 0.1952, 0.0000, 0.0000, 0.2241, 0.3541, 0.2932, 0.1462, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.0990, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0049, 0.1255, 0.3400, 0.1416, 0.0000, 0.0000, 0.1024,\n",
      "         0.2253, 0.0978, 0.0000, 0.0207, 0.1631, 0.2250, 0.1769, 0.1131, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.0870, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2760, 0.5707, 0.5405, 0.0641, 0.0000, 0.1351,\n",
      "         0.2727, 0.1704, 0.0579, 0.0998, 0.1212, 0.1336, 0.1107, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.0997, 0.0058, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2014, 0.7477, 0.6992, 0.2553, 0.0000, 0.1956,\n",
      "         0.2980, 0.1953, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.0744, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0279, 0.7211, 1.1177, 0.7426, 0.3322, 0.2460,\n",
      "         0.2641, 0.1222, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1013, 0.0976,\n",
      "         0.0297, 0.0000, 0.0000, 0.0000, 0.4867, 0.9109, 1.0995, 0.9310, 0.6953,\n",
      "         0.4336, 0.2078, 0.1228, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.0502, 0.0000, 0.0000, 0.0000, 0.0000, 0.3352, 0.6396, 0.9544, 1.0047,\n",
      "         0.7690, 0.4847, 0.2386, 0.1241, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1025, 0.0223, 0.0000, 0.0000, 0.0000, 0.0000, 0.0628, 0.4941, 0.8487,\n",
      "         1.0276, 0.7735, 0.5331, 0.2353, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.0927, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1331, 0.4865,\n",
      "         0.9571, 1.0572, 0.7692, 0.4034, 0.1474, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.0740, 0.0000, 0.0000, 0.0000, 0.0000, 0.0647, 0.4965,\n",
      "         0.8723, 0.9974, 0.8749, 0.4027, 0.1330, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1152, 0.1735, 0.2705, 0.2855, 0.2862, 0.2531, 0.2868, 0.4703,\n",
      "         0.8240, 1.0557, 0.8257, 0.4106, 0.1131, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1120,\n",
      "         0.1527, 0.2548, 0.3626, 0.4569, 0.6022, 0.6628, 0.5230, 0.1885, 0.1500,\n",
      "         0.4438, 0.8474, 0.6977, 0.3257, 0.1344, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1118, 0.1418, 0.2116,\n",
      "         0.3589, 0.4167, 0.5405, 0.6459, 0.6135, 0.5262, 0.3406, 0.1184, 0.0000,\n",
      "         0.1067, 0.3431, 0.4248, 0.2688, 0.1139, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1108, 0.1591, 0.3082, 0.3848, 0.5502,\n",
      "         0.6116, 0.6544, 0.6230, 0.4939, 0.3497, 0.2204, 0.0837, 0.0000, 0.0000,\n",
      "         0.1859, 0.3951, 0.3766, 0.2051, 0.1072, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1185, 0.1932, 0.3174, 0.4138, 0.5243, 0.7012, 0.7655,\n",
      "         0.7262, 0.5611, 0.3828, 0.2408, 0.1075, 0.0000, 0.0000, 0.0542, 0.1178,\n",
      "         0.1761, 0.2496, 0.2630, 0.1242, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1263, 0.2307, 0.3243, 0.4794, 0.6445, 0.5839, 0.4821,\n",
      "         0.3257, 0.1025, 0.0428, 0.0276, 0.0187, 0.0705, 0.1004, 0.1312, 0.1900,\n",
      "         0.2270, 0.1693, 0.1144, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.0603, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0341, 0.1089, 0.1332, 0.1916, 0.2226, 0.1724,\n",
      "         0.1158, 0.1072, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.0469, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0986, 0.1686, 0.2197, 0.2399, 0.1681, 0.1195, 0.1079,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.0843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0215, 0.0459,\n",
      "         0.0084, 0.0708, 0.1093, 0.2328, 0.2025, 0.1256, 0.1081, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.0515, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0716, 0.1089, 0.1818, 0.1282, 0.1085, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070],\n",
      "        [0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070, 0.1070,\n",
      "         0.1070]])\n"
     ]
    }
   ],
   "source": [
    "print(conv1[0][0].size())\n",
    "print(conv1[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(conv1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0\n",
      "1 : 0\n",
      "2 : 0\n",
      "3 : 0\n",
      "4 : 0\n",
      "5 : 0\n",
      "6 : 0\n",
      "7 : 0\n",
      "8 : 0\n",
      "9 : 0\n",
      "10 : 0\n",
      "11 : 0\n",
      "12 : 0\n",
      "13 : 0\n",
      "14 : 0\n",
      "15 : 0\n",
      "16 : 0\n",
      "17 : 0\n",
      "18 : 0\n",
      "19 : 0\n",
      "20 : 0\n",
      "21 : 0\n",
      "22 : 0\n",
      "23 : 0\n",
      "24 : 0\n",
      "25 : 0\n",
      "26 : 0\n",
      "27 : 0\n",
      "28 : 0\n",
      "29 : 0\n",
      "30 : 0\n",
      "31 : 0\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "f_cnt = -1\n",
    "for f in ft_list[:32]:\n",
    "    f_cnt += 1\n",
    "    if f.all() == 0:\n",
    "        print(f_cnt, ':', 0)\n",
    "        continue\n",
    "    d_cnt = -1\n",
    "    for d in conv1[0]:\n",
    "        d_cnt += 1\n",
    "        try:\n",
    "            print(torch.eq(h, d.to(\"cuda\")).all())\n",
    "            condition = torch.eq(h, d.to(\"cuda\")).all() == True \n",
    "            if condition:\n",
    "                print(\"equeals\", f_cnt, d_cnt)\n",
    "                cnt += 1\n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('mntd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d7768e61f5674adf4efa61c7b8cc3ee2c06ae8f502b5df709cd3e31381a4347"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
